{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5492be11-d50a-44c8-9400-ca21d3f99251",
   "metadata": {},
   "source": [
    "# TF-IDF Search Using Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47062a6-4ec6-4118-b7c0-3b0b71e5166c",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "242ebd8d-f002-456e-85d2-329672d1932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter, OrderedDict\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233e987f-1470-4a81-bdab-c1ce8105a20d",
   "metadata": {},
   "source": [
    "## Open data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63dab127-1bbd-4484-989c-432212f0a6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tokenized_data.json\", \"r\") as read_file:\n",
    "    CDC_data = json.load(read_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f606fe-a5ee-44e5-af1e-751a6cb82d95",
   "metadata": {},
   "source": [
    "## Create corpus vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d664e448-2828-42d7-8c57-470f7a1f61ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vocabulary(corpus: List[dict]) -> List[str]:\n",
    "    \"\"\"Determine the vocabulary, i.e. the sorted list of all unique tokens, for a corpus of lemmatized articles.\"\"\"\n",
    "    all_corpus_tokens = []\n",
    "    for _, article in enumerate(corpus):\n",
    "        all_corpus_tokens += article.get(\"tokenized_text\")\n",
    "    vocabulary = sorted(list(set(all_corpus_tokens)))\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "befaf2a6-0b9a-4e6b-bef7-8ca5241c19fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "CDC_vocabulary = compute_vocabulary(corpus = CDC_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "506194bb-7257-48b0-be93-749d3afe1a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1484"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CDC_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4c417ab-e05f-4f36-98ed-535bf5df108d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('CDC_vocabulary.json', 'w') as out:\n",
    "    json.dump(CDC_vocabulary, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533ddd42-41cf-4018-84da-8f927bfb963a",
   "metadata": {},
   "source": [
    "## TF-builder function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3792f162-19f5-4700-b89e-9d87ca51b212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_TF(vocabulary: list, tokenized_text: List[str]) -> 'OrderedDict[str, float]':\n",
    "    \"\"\"Compute TF given a vocabulary and a tokenized document, and return the documentâ€™s TF-vector.\"\"\"\n",
    "    counter = Counter(tokenized_text)\n",
    "    normalizer = sum(counter.values())\n",
    "    TF_vector = OrderedDict((token, 0) for token in vocabulary)\n",
    "    for key, value in counter.items():\n",
    "        TF_vector[key] = value/normalizer\n",
    "    return TF_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d6bd4bc-ebd8-4bbd-8ddb-8bc34b65d7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_TF = compute_TF(vocabulary=CDC_vocabulary, tokenized_text=CDC_data[2].get('tokenized_text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a42f646c-89cb-4ab2-a1f6-f68cd9c66636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06153846153846154"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_TF['plague']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d952f37f-ccf7-4669-9fc2-5ecb8c591d23",
   "metadata": {},
   "source": [
    "## IDF-builder function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28602beb-292f-496f-9b0e-24e5631f5b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_IDF(vocabulary: list, corpus: List[dict]) -> 'OrderedDict[str, float]':\n",
    "    \"\"\"Compute DF for each word in the vocabulary given a corpus of tokenized texts.\"\"\"\n",
    "    IDF_vector = OrderedDict((token, 0) for token in vocabulary)\n",
    "    corpus_size = len(corpus)\n",
    "    for key in IDF_vector.keys():\n",
    "        for _, article in enumerate(corpus):\n",
    "            IDF_vector[key] += (key in article['tokenized_text'])\n",
    "        IDF_vector[key] = np.log(corpus_size/IDF_vector[key])      \n",
    "    return IDF_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0dea018-2bcc-4b69-bd23-2bc2672de868",
   "metadata": {},
   "outputs": [],
   "source": [
    "CDC_IDF = compute_IDF(vocabulary=CDC_vocabulary, corpus=CDC_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1131cac3-be47-4699-98b3-efe9d7382069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.159484249353372"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CDC_IDF['plague']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a9cf52-7936-4fee-97a2-b5bc08ae44fc",
   "metadata": {},
   "source": [
    "## Add TF_IDF field to CDC corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d82759f4-a22b-4dea-977a-fd8168cf7ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_TF_IDF(corpus: List[dict]) -> List[dict]:\n",
    "    \"\"\"Add a TF_IDF_vector field to each article in a given lemmatized corpus.\"\"\"\n",
    "    vocabulary = compute_vocabulary(corpus)\n",
    "    IDF_vector = compute_IDF(vocabulary=vocabulary, corpus=corpus)\n",
    "    for _, article in enumerate(corpus):\n",
    "        article['tf_idf'] = compute_TF(vocabulary=vocabulary, tokenized_text=article['tokenized_text'])\n",
    "        article['tf_idf'] = OrderedDict({key: value*IDF_vector[key] for key, value in article['tf_idf'].items()})\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b5fde4e-5982-4445-a901-6579dfde2095",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data = compute_TF_IDF(corpus=CDC_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c026ce3e-9c7d-4474-a2b2-632502a82645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.132891338421746"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_data[2]['tf_idf']['plague']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effb11b9-375d-417d-9d13-3f391bbd325c",
   "metadata": {},
   "source": [
    "## Save corpus with TF_IDF vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d2c4300-16ce-4fad-82fd-886af3d72345",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vectorized_data.json', 'w') as out:\n",
    "    json.dump(vectorized_data, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7065e8-4f19-40bb-93c6-b8c9f69353c6",
   "metadata": {},
   "source": [
    "## Query comparison to corpus with cosine similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9a6cb67-8668-43c4-87a4-91209a20afd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF_IDF_dict_to_array(corpus: List[dict]) -> np.ndarray:\n",
    "    \"\"\"Given a corpus, create the array of tf-idf vectors for the whole corpus.\"\"\"\n",
    "    TF_IDF_list = []\n",
    "    for _, article in enumerate(corpus):\n",
    "        TF_IDF_list.append(list(article.get('tf_idf').values()))\n",
    "    return np.asarray(TF_IDF_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8865c4f7-2caf-422c-88fc-e5b1edcb326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_tfidf(tfidf_dict: 'OrderedDict[str, float]') -> np.ndarray:\n",
    "    return np.array(list(tfidf_dict.values())).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "461e9836-6e50-4c97-8d12-3632fc354407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity_to_corpus(query: dict, corpus: List[dict]):\n",
    "    \"\"\"Given a query article, compute its cosine similarity to each article in the corpus.\"\"\"\n",
    "    corpus_tfidf = TF_IDF_dict_to_array(corpus)\n",
    "    distances = cosine_similarity(reshape_tfidf(query.get('tf_idf')), corpus_tfidf)\n",
    "    return pd.Series(distances.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a8ab7eb-9b46-4f3f-ad96-00c2a238f361",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_to_corpus_2 = compute_similarity_to_corpus(query=vectorized_data[2], corpus=vectorized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1b76e26-90cd-4890-94d1-ebf965dec569",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_to_corpus_10 = compute_similarity_to_corpus(query=vectorized_data[10], corpus=vectorized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fb981d9-0eea-4184-b858-aa99d001938e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2     1.000000\n",
       "15    0.349125\n",
       "0     0.074371\n",
       "11    0.065140\n",
       "10    0.031332\n",
       "5     0.031121\n",
       "6     0.030052\n",
       "4     0.028216\n",
       "1     0.027421\n",
       "7     0.026526\n",
       "17    0.024144\n",
       "3     0.017936\n",
       "19    0.016914\n",
       "25    0.014236\n",
       "8     0.012659\n",
       "21    0.011262\n",
       "23    0.010593\n",
       "20    0.008291\n",
       "12    0.006328\n",
       "22    0.006017\n",
       "16    0.005696\n",
       "18    0.005475\n",
       "13    0.004698\n",
       "14    0.002994\n",
       "9     0.001559\n",
       "24    0.001139\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances_to_corpus_2.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80c04fa2-aa8c-40ab-b2dd-a24d89c0a3ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10    1.000000\n",
       "1     0.180081\n",
       "11    0.151779\n",
       "25    0.104621\n",
       "6     0.089411\n",
       "5     0.089371\n",
       "0     0.085912\n",
       "24    0.082944\n",
       "19    0.074112\n",
       "21    0.063049\n",
       "16    0.058468\n",
       "12    0.049429\n",
       "20    0.037985\n",
       "17    0.036746\n",
       "14    0.035690\n",
       "13    0.035402\n",
       "2     0.031332\n",
       "22    0.029978\n",
       "8     0.028662\n",
       "3     0.027454\n",
       "23    0.024984\n",
       "7     0.024258\n",
       "18    0.022718\n",
       "15    0.022150\n",
       "9     0.015973\n",
       "4     0.008713\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances_to_corpus_10.sort_values(ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLP_project)",
   "language": "python",
   "name": "nlp_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
